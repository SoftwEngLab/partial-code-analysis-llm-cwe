{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ae6a5e-3d27-4538-99fb-e4b6eef775b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b55253-57bb-41ca-bae2-fe3305462116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from openai import OpenAI\n",
    "\n",
    "def query_openai(prompt, model, temperature, mode):\n",
    "    \"\"\"\n",
    "    Queries the OpenAI API or Hugging Face API depending on the mode.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if mode == \"deepseek\":\n",
    "            client = OpenAI(\n",
    "                base_url=\"https://huggingface.co/api/inference-proxy/together\",\n",
    "                api_key=\"\"\n",
    "            )\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=150,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stream=False\n",
    "            )\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-azure\":\n",
    "            os.environ['ENDPOINT'] = 'https://aistudioaiservices133462205434.services.ai.azure.com/models'\n",
    "            os.environ['API_KEY'] = ''\n",
    "            os.environ['MODEL'] = 'DeepSeek-R1'\n",
    "            \n",
    "            endpoint = os.environ[\"ENDPOINT\"]\n",
    "            api_key = os.environ[\"API_KEY\"]\n",
    "            model_name = os.environ[\"MODEL\"]\n",
    "            \n",
    "            client = ChatCompletionsClient(\n",
    "                endpoint=endpoint,\n",
    "                credential=AzureKeyCredential(api_key),\n",
    "            )\n",
    "            \n",
    "            response = client.complete(\n",
    "                messages=[\n",
    "                    SystemMessage(content=\"You are a helpful assistant. Limit your response to 130 tokens.\"),\n",
    "                    UserMessage(content=prompt),\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                model=model_name\n",
    "            )\n",
    "\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-nvidia\":\n",
    "            client = OpenAI(\n",
    "                base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "                api_key=\"nvapi-\",\n",
    "            )\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "              model=\"deepseek-ai/deepseek-r1\",\n",
    "              messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "              temperature=temperature,\n",
    "              top_p=1,\n",
    "              stream=False\n",
    "            )\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-api\":\n",
    "            client = OpenAI(api_key=\"sk-\", base_url=\"https://api.deepseek.com\")\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-reasoner\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "              top_p=1,\n",
    "              stream=False\n",
    "            )\n",
    "            print(response)\n",
    "            return response\n",
    "            \n",
    "        elif mode == \"hf_endpoint\":\n",
    "            return query_hf_api(prompt, temperature)\n",
    "        elif mode == \"o1\":\n",
    "            print(\"......\", mode)\n",
    "            client = openai\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 500 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ])\n",
    "            return response\n",
    "        else:\n",
    "            client = openai\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=150,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stream=False\n",
    "            )\n",
    "            return response\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API query: {e}\")\n",
    "    return None\n",
    "\n",
    "def query_hf_api(prompt, temperature):\n",
    "    \"\"\"\n",
    "    Queries the Hugging Face API.\n",
    "    \"\"\"\n",
    "    # payload = {\n",
    "    #     \"inputs\": prompt,\n",
    "    #     \"parameters\": {\n",
    "    #         \"temperature\": temperature,\n",
    "    #         \"max_new_tokens\": 150\n",
    "    #     }\n",
    "    #}\n",
    "    try:\n",
    "        #response = requests.post(HF_API_URL, headers=HF_HEADERS, json=payload)\n",
    "        #response.raise_for_status()\n",
    "        #return response.json()\n",
    "\n",
    "        # define client openai with the endpoint server url\n",
    "        client = OpenAI(\n",
    "        \tbase_url=\"https://ue4xb2y39sfo803n.us-east-1.aws.endpoints.huggingface.cloud/v1/\", \n",
    "            api_key=\"\" \n",
    "            )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"tgi\",\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            top_p=None,\n",
    "            temperature=None,\n",
    "            max_tokens=1000,\n",
    "            stream=False,\n",
    "            seed=None,\n",
    "            stop=None,\n",
    "            frequency_penalty=None,\n",
    "            presence_penalty=None\n",
    "            )\n",
    "        \n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while querying the Hugging Face API: {e}\")\n",
    "    return None\n",
    "def extract_response_content(response):\n",
    "    \"\"\"\n",
    "    Extracts the content from the API response for serialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return response.choices[0].message.content if response else None\n",
    "    except (KeyError, TypeError, AttributeError) as e:\n",
    "        print(f\"Error extracting response content: {e}\")\n",
    "        print(\"Full response:\", response)\n",
    "    return None\n",
    "\n",
    "def read_prompt_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the prompt from a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_responses_to_json(responses, folder_path, model, temperature, trial):\n",
    "    \"\"\"\n",
    "    Saves all responses to a single JSON file.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file_name = f\"output_responses_{model.replace('/', '_')}_temp{temperature}_{timestamp}_trial_{trial}.json\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    try:\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            json.dump(responses, file, indent=4)\n",
    "        print(f\"------------Responses saved to {output_file_path}---------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the file: {e}\")\n",
    "def save_response_to_json(response_content, folder_path, model, trial, filename):\n",
    "    \"\"\"\n",
    "    Saves the response content to a JSON file, ensuring no overwrite.\n",
    "    \"\"\"\n",
    "    output_file_name = f\"{filename.replace('.txt', '_response')}_{model.replace('/', '_')}_trial_{trial}.json\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    \n",
    "    if not os.path.exists(output_file_path):\n",
    "        try:\n",
    "            with open(output_file_path, 'w') as file:\n",
    "                json.dump({filename: response_content}, file, indent=4)\n",
    "            print(f\"Response saved to {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save the file: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping {filename}: Response file already exists.\")\n",
    "\n",
    "def process_file(filename, folder_path, model, temperature, trial, mode):\n",
    "    \"\"\"\n",
    "    Processes a single file, checking if a response already exists before querying.\n",
    "    \"\"\"\n",
    "    output_file_name = f\"{filename.replace('.txt', '_response')}_{model.replace('/', '_')}_trial_{trial}.json\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    \n",
    "    if os.path.exists(output_file_path):\n",
    "        print(f\"Skipping {filename}: Response already exists in {output_file_path}\")\n",
    "        return filename, None\n",
    "    \n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    prompt_text = read_prompt_from_file(file_path)\n",
    "    \n",
    "    if prompt_text:\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        response = query_openai(prompt_text, model, temperature, mode)\n",
    "        content = extract_response_content(response)\n",
    "        \n",
    "        if content:\n",
    "            save_response_to_json(content, folder_path, model, trial, filename)\n",
    "            return filename, content\n",
    "    return filename, None\n",
    "\n",
    "def process_folder_parallel(folder_path, model, temperature, trial, mode, max_workers=10):\n",
    "    \"\"\"\n",
    "    Processes all eligible text files in the folder in parallel.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if re.match(r'output_sample_(\\d+)\\.txt', f)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(process_file, file, folder_path, model, temperature, trial, mode): file for file in files[::-1]}\n",
    "        for future in as_completed(future_to_file):\n",
    "            future.result()\n",
    "\n",
    "def combine_all_responses(folder_path, model, temperature, trial):\n",
    "    \"\"\"\n",
    "    Combines all response JSONs into a single JSON file only if every file \n",
    "    matching output_sample_{n}.txt has a corresponding \n",
    "    output_sample_{n}_response_{model}_trial_{trial}.json.\n",
    "    \"\"\"\n",
    "    responses = {}\n",
    "    pattern = re.compile(r\"^output_sample_(\\d+)\\.txt$\")  # Matches files like output_sample_1.txt\n",
    "\n",
    "    # Get all filenames in the folder\n",
    "    files_in_folder = set(os.listdir(folder_path))\n",
    "\n",
    "    # Collect all `output_sample_{n}.txt` and check if each has a corresponding JSON\n",
    "    txt_files = {file for file in files_in_folder if pattern.match(file)}\n",
    "    json_files = {\n",
    "        file.replace(\".txt\", f\"_response_{model.replace('/', '_')}_trial_{trial}.json\")\n",
    "        for file in txt_files\n",
    "    }\n",
    "\n",
    "    # Check if all expected JSON files exist\n",
    "    if not json_files.issubset(files_in_folder):\n",
    "        print(\">>>>>>>>>>>>>>>Not all .txt files have their corresponding .json files. Aborting combination.<<<<<<<<<<<<<<<\")\n",
    "        return\n",
    "\n",
    "    # If all JSON files exist, process them\n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(folder_path, json_file)\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                responses.update(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_file}: {e}\")\n",
    "    \n",
    "    save_responses_to_json(responses, folder_path, model, temperature, trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf6eb3f-67da-434a-92fe-70af782cd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b2c2ecb-8afb-41d3-9b50-377b896786d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before deepseek-ai/DeepSeek-R1 0.6 1 deepseek-r1-nvidia 1\n",
      "Skipping output_sample_16.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_16_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_17.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_17_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_15.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_15_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_29.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_29_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_28.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_28_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_14.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_14_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_38.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_38_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_10.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_10_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_11.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_11_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_39.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_39_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_13.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_13_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_12.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_12_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_61.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_61_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_49.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_49_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_48.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_48_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_60.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_60_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_1.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_1_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_3.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_3_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_62.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_62_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_63.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_63_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_2.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_2_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_6.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_6_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_67.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_67_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_66.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_66_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_7.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_7_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_5.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_5_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_58.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_58_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_64.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_64_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_70.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_70_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_65.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_65_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_59.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_59_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_4.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_4_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_9.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_9_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_40.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_40_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_54.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_54_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_68.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_68_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_69.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_69_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_55.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_55_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_41.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_41_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_8.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_8_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_57.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_57_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_43.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_43_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_42.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_42_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_56.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_56_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_52.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_52_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_46.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_46_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_53.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_53_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_45.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_45_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_51.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_51_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_50.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_50_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_44.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_44_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_23.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_23_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_37.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_37_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_36.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_36_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_22.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_22_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_34.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_34_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_20.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_20_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_21.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_21_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_35.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_35_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_19.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_19_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_31.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_31_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_25.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_25_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_24.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_24_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_30.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_30_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_18.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_18_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_26.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_26_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_32.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_32_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_33.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_33_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_27.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_sample_27_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "------------Responses saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_before/output_responses_deepseek-ai_DeepSeek-R1_temp0.6_20250208061240_trial_1.json---------\n",
      "/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after deepseek-ai/DeepSeek-R1 0.6 1 deepseek-r1-nvidia 1\n",
      "Skipping output_sample_16.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_16_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Processing file: output_sample_17.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_17_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_15.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_15_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_29.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_29_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_28.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_28_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_14.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_14_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_38.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_38_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_10.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_10_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_11.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_11_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_39.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_39_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_13.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_13_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_12.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_12_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_61.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_61_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Processing file: output_sample_49.txt\n",
      "Skipping output_sample_49.txt: Response file already exists.\n",
      "Processing file: output_sample_48.txt\n",
      "Skipping output_sample_48.txt: Response file already exists.\n",
      "Skipping output_sample_60.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_60_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_1.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_1_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Processing file: output_sample_3.txt\n",
      "Skipping output_sample_62.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_62_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_63.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_63_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Processing file: output_sample_2.txt\n",
      "Skipping output_sample_2.txt: Response file already exists.\n",
      "Skipping output_sample_6.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_6_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_67.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_67_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_66.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_66_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Processing file: output_sample_7.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 252\u001b[0m, in \u001b[0;36mprocess_folder_parallel\u001b[0;34m(folder_path, model, temperature, trial, mode, max_workers)\u001b[0m\n\u001b[1;32m    251\u001b[0m future_to_file \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(process_file, file, folder_path, model, temperature, trial, mode): file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]}\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(future_to_file):\n\u001b[1;32m    253\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m combine_all_responses(folder_path_before, model, temperature, trial)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(folder_path_after, model, temperature, trial, mode, mw)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mprocess_folder_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path_after\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m combine_all_responses(folder_path_after, model, temperature, trial)\n",
      "Cell \u001b[0;32mIn[10], line 253\u001b[0m, in \u001b[0;36mprocess_folder_parallel\u001b[0;34m(folder_path, model, temperature, trial, mode, max_workers)\u001b[0m\n\u001b[1;32m    251\u001b[0m future_to_file \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(process_file, file, folder_path, model, temperature, trial, mode): file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]}\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(future_to_file):\n\u001b[0;32m--> 253\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/concurrent/futures/_base.py:628\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/concurrent/futures/thread.py:229\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 229\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/threading.py:1029\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/threading.py:1045\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1046\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping output_sample_7.txt: Response file already exists.\n",
      "Processing file: output_sample_5.txt\n",
      "Skipping output_sample_5.txt: Response file already exists.\n",
      "Skipping output_sample_58.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_58_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_64.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_64_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_70.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_70_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_65.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_65_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_59.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_59_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_4.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_4_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_9.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_9_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_40.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_40_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_54.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_54_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_68.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_68_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_69.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_69_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_55.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_55_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_41.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_41_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_8.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_8_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_57.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_57_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_43.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_43_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_42.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_42_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_56.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_56_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_52.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_52_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_46.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_46_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_53.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_53_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_45.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_45_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_51.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_51_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_50.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_50_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_44.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_44_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_23.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_23_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_37.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_37_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_36.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_36_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_22.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_22_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_34.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_34_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_20.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_20_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_21.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_21_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_35.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_35_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_19.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_19_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_31.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_31_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_25.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_25_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_24.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_24_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_30.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_30_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_18.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_18_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_26.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_26_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_32.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_32_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_33.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_33_response_deepseek-ai_DeepSeek-R1_trial_1.json\n",
      "Skipping output_sample_27.txt: Response already exists in /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_16/func_src_after/output_sample_27_response_deepseek-ai_DeepSeek-R1_trial_1.json\n"
     ]
    }
   ],
   "source": [
    "cwe_list = [\"cwe-476\"]\n",
    "experiment_list = [\"exp_16\"]\n",
    "\n",
    "for cwe in cwe_list:\n",
    "    for exp in experiment_list:\n",
    "        folder_path_before = f\"/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/{cwe}/{exp}/func_src_before\"\n",
    "        folder_path_after = f\"/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/{cwe}/{exp}/func_src_after\"\n",
    "        #model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
    "        #model = \"o1\"\n",
    "        model = \"deepseek-ai/DeepSeek-R1\"\n",
    "        temperature = 0.6\n",
    "        trial = 1\n",
    "        mw = 10\n",
    "        mode = \"openai\"\n",
    "        if model == \"deepseek-ai/DeepSeek-R1\":\n",
    "            mode = \"deepseek-r1-nvidia\"\n",
    "            mw = 1\n",
    "        elif model == \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\":\n",
    "            mode = \"hf_endpoint\"\n",
    "            mw = 4\n",
    "        elif model == \"o1\":\n",
    "            mode = \"o1\"\n",
    "            mw = 3\n",
    "\n",
    "        print(folder_path_before, model, temperature, trial, mode, mw)\n",
    "        process_folder_parallel(folder_path_before, model, temperature, trial, mode, max_workers=mw)\n",
    "        combine_all_responses(folder_path_before, model, temperature, trial)\n",
    "\n",
    "        print(folder_path_after, model, temperature, trial, mode, mw)\n",
    "        process_folder_parallel(folder_path_after, model, temperature, trial, mode, max_workers=mw)\n",
    "        combine_all_responses(folder_path_after, model, temperature, trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a57d2-d602-4df3-9b3b-b997a35e2335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf19d29-fb04-4a98-aba6-7c18973fb0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514f4a0-f432-413e-8e44-b88e380ca00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b91c6b-db58-47d2-acce-d5844de6c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be36ef-34e4-45f6-9ae2-6cbcaabbff43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2facbdcd-7776-431b-b415-d7bf433f4025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99547ed9-f9ba-40e6-9f70-668e93cec284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f84e76-8605-4a60-b803-741b8eaea764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab9044-1934-48e3-b04a-c93df4cde611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mw = 10\n",
    "mode = \"openai\"\n",
    "if model == \"deepseek-ai/DeepSeek-R1\":\n",
    "    mode = \"deepseek-r1\"\n",
    "    mw = 5\n",
    "elif model == \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\":\n",
    "    mode = \"hf_endpoint\"\n",
    "    mw = 4\n",
    "elif model == \"o1\":\n",
    "    mode = \"o1\"\n",
    "    mw = 3\n",
    "\n",
    "    \n",
    "print(folder_path, model, temperature, trial, mode, mw)\n",
    "process_folder_parallel(folder_path, model, temperature, trial, mode, max_workers=mw)\n",
    "combine_all_responses(folder_path, model, temperature, trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63d899-033b-4f89-8a48-1f2b5fab979e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa8072-32ee-4ba8-9788-fd507d29711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key=\"nvapi-\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"deepseek-ai/deepseek-r1\",\n",
    "  messages=[{\"role\":\"user\",\"content\":\"Which number is larger, 9.11 or 9.8?\"}],\n",
    "  temperature=0.6,\n",
    "  top_p=0.7,\n",
    "  max_tokens=4096,\n",
    "  stream=False\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ece667-f736-4b9c-8744-da1b0981cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d568907-40e6-4e60-801d-b3961092197f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e986b-c4af-48a1-95a8-7d6d7748efe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f252c5-c4a1-4499-af10-663f361aef6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a71baa-1965-4011-a0e6-f73e714dc0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be993df4-1fbe-47f3-9122-5ed331258974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30015b5d-18c2-419c-b707-24df7d7d4683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0878265-e79d-4a6c-b203-bb24747f9171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df72baca-49e6-4b58-a49f-c124d6ec659a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e7207-6241-49e4-bd05-e4faa3148027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, install the openai Python library by running \n",
    "# pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://ue4xb2y39sfo803n.us-east-1.aws.endpoints.huggingface.cloud/v1/\", \n",
    "\tapi_key=\"\" \n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "\tmodel=\"tgi\",\n",
    "\tmessages=[\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is deep learning?\"\n",
    "\t}\n",
    "],\n",
    "\ttop_p=None,\n",
    "\ttemperature=None,\n",
    "\tmax_tokens=150,\n",
    "\tstream=False,\n",
    "\tseed=None,\n",
    "\tstop=None,\n",
    "\tfrequency_penalty=None,\n",
    "\tpresence_penalty=None\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)\n",
    "#for message in chat_completion:\n",
    "#\tprint(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7568d-9e45-4793-b224-c920ecfd2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message.choices[0].delta.content, end=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ab527-ec20-4b79-99f9-cba883d894ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(prompt, model, temperature, mode):\n",
    "    \"\"\"\n",
    "    Queries the OpenAI API with the specified parameters and returns the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai\n",
    "\n",
    "        if mode == \"deepseek\":\n",
    "            client = OpenAI(\n",
    "                base_url=\"https://huggingface.co/api/inference-proxy/together\",\n",
    "                api_key=\"\"\n",
    "            )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant. You must LIMIT YOUR RESPONSE TO 130 TOKENS.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=130,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stream=False\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API query: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_response_content(response):\n",
    "    \"\"\"\n",
    "    Extracts the content from the API response for serialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return response.choices[0].message.content\n",
    "    except (KeyError, TypeError) as e:\n",
    "        print(f\"Error extracting response content: {e}\")\n",
    "        print(\"Full response:\", response)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0174faa-73d6-465d-b6fa-a5e1611699cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =\"deepseek-ai/DeepSeek-R1\"\n",
    "mode = \"deepseek\"\n",
    "temperature = 0.7\n",
    "response = query_openai(\"What is the capital of France?\", model, temperature, mode)\n",
    "content = extract_response_content(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b5c2a-adf7-4655-8883-e24607c6c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac72a8c-eac9-4861-9972-88b3a249938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://huggingface.co/api/inference-proxy/together\",\n",
    "\tapi_key=\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "\tmodel=\"deepseek-ai/DeepSeek-R1\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a8b24-f5bf-4782-a41f-3d0f729a3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027c726-dc7b-4277-94ce-9a1999df63b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
