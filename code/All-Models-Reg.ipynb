{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ae6a5e-3d27-4538-99fb-e4b6eef775b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ic/.pyenv/versions/3.9.0/envs/nlp/lib/python3.9/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import litellm\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import requests\n",
    "from together import Together\n",
    "# Global variable for Vertex AI credential\n",
    "VERTEX_AI_CREDENTIAL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3a2ccb-766f-45b6-a209-d90deae03430",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09c6cc0-8847-473f-967e-aa64affabb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_litellm(prompt, llm_model, temperature):\n",
    "    \"\"\"\n",
    "    Queries the VertexAI API with the specified parameters and returns the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=llm_model,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant. You must LIMIT YOUR RESPONSE TO 130 TOKENS.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            vertex_credentials=VERTEX_AI_CREDENTIAL,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API query: {e}\")\n",
    "    return None\n",
    "    \n",
    "def query_openai_o1(prompt, model, temperature):\n",
    "    client = openai\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 500 tokens.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ])\n",
    "    return response\n",
    "    \n",
    "def query_openai(prompt, model, temperature, mode):\n",
    "    \"\"\"\n",
    "    Queries the OpenAI API or Hugging Face API depending on the mode.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if mode == \"deepseek\":\n",
    "            client = OpenAI(\n",
    "                base_url=\"https://huggingface.co/api/inference-proxy/together\",\n",
    "                api_key=\"\"\n",
    "            )\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=150,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stream=False\n",
    "            )\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-together\":\n",
    "            os.environ['TOGETHER_API_KEY'] = ''\n",
    "            client = Together()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-ai/DeepSeek-R1\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                top_p=1,\n",
    "            )\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-deepinfra\":\n",
    "\n",
    "            # Create an OpenAI client with your deepinfra token and endpoint\n",
    "            openai = OpenAI(\n",
    "                api_key=\"\",\n",
    "                base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "            )\n",
    "            \n",
    "            # chat_completion = openai.chat.completions.create(\n",
    "            #     model=\"deepseek-ai/DeepSeek-R1\",\n",
    "            #     messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "            # )\n",
    "\n",
    "            response = openai.chat.completions.create(\n",
    "              model=\"deepseek-ai/DeepSeek-R1\",\n",
    "              messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "              temperature=temperature,\n",
    "              top_p=1,\n",
    "              stream=False\n",
    "            )\n",
    "\n",
    "            return response\n",
    "\n",
    "        elif mode == \"deepseek-r1-github\":          \n",
    "            print(\"...github!\")\n",
    "            endpoint = \"https://models.inference.ai.azure.com\"\n",
    "            model_name = \"DeepSeek-R1\"\n",
    "            \n",
    "            os.environ['GITHUB_TOKEN'] = ''\n",
    "            token = os.environ[\"GITHUB_TOKEN\"]\n",
    "            \n",
    "            client = ChatCompletionsClient(\n",
    "                endpoint=endpoint,\n",
    "                credential=AzureKeyCredential(token),\n",
    "            )\n",
    "            \n",
    "            response = client.complete(\n",
    "                messages=[\n",
    "                    SystemMessage(content=\"You are a helpful assistant. Limit your response to 130 tokens.\"),\n",
    "                    UserMessage(content=prompt),\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                model=model_name\n",
    "            )\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-azure\":\n",
    "            os.environ['ENDPOINT'] = 'https://aistudioaiservices133462205434.services.ai.azure.com/models'\n",
    "            os.environ['API_KEY'] = ''\n",
    "            os.environ['MODEL'] = 'DeepSeek-R1'\n",
    "            \n",
    "            endpoint = os.environ[\"ENDPOINT\"]\n",
    "            api_key = os.environ[\"API_KEY\"]\n",
    "            model_name = os.environ[\"MODEL\"]\n",
    "            \n",
    "            client = ChatCompletionsClient(\n",
    "                endpoint=endpoint,\n",
    "                credential=AzureKeyCredential(api_key),\n",
    "            )\n",
    "            \n",
    "            response = client.complete(\n",
    "                messages=[\n",
    "                    SystemMessage(content=\"You are a helpful assistant. Limit your response to 130 tokens.\"),\n",
    "                    UserMessage(content=prompt),\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "                model=model_name\n",
    "            )\n",
    "\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-nvidia\":\n",
    "            client = OpenAI(\n",
    "                base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "                api_key=\"nvapi-\",\n",
    "            )\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "              model=\"deepseek-ai/deepseek-r1\",\n",
    "              messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "              temperature=temperature,\n",
    "              top_p=1,\n",
    "              stream=False\n",
    "            )\n",
    "            return response\n",
    "        elif mode == \"deepseek-r1-api\":\n",
    "            client = OpenAI(api_key=\"sk-\", base_url=\"https://api.deepseek.com\")\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-reasoner\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "              top_p=1,\n",
    "              stream=False\n",
    "            )\n",
    "            print(response)\n",
    "            return response\n",
    "            \n",
    "        elif mode == \"hf_endpoint\":\n",
    "            return query_hf_api(prompt, temperature)\n",
    "        elif mode == \"o1\":\n",
    "            return query_openai_o1(prompt, model, temperature)\n",
    "        elif mode == \"bedrock\":\n",
    "            return query_litellm(prompt, model, temperature)\n",
    "        else:\n",
    "            print(\"got here\")\n",
    "            client = openai\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=150,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stream=False\n",
    "            )\n",
    "            return response\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API query: {e}\")\n",
    "    return None\n",
    "\n",
    "def query_hf_api(prompt, temperature):\n",
    "    \"\"\"\n",
    "    Queries the Hugging Face API.\n",
    "    \"\"\"\n",
    "    # payload = {\n",
    "    #     \"inputs\": prompt,\n",
    "    #     \"parameters\": {\n",
    "    #         \"temperature\": temperature,\n",
    "    #         \"max_new_tokens\": 150\n",
    "    #     }\n",
    "    #}\n",
    "    try:\n",
    "        #response = requests.post(HF_API_URL, headers=HF_HEADERS, json=payload)\n",
    "        #response.raise_for_status()\n",
    "        #return response.json()\n",
    "\n",
    "        # define client openai with the endpoint server url\n",
    "        client = OpenAI(\n",
    "        \tbase_url=\"https://ue4xb2y39sfo803n.us-east-1.aws.endpoints.huggingface.cloud/v1/\", \n",
    "            api_key=\"\" \n",
    "            )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"tgi\",\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Limit your response to 130 tokens.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            top_p=None,\n",
    "            temperature=None,\n",
    "            max_tokens=1000,\n",
    "            stream=False,\n",
    "            seed=None,\n",
    "            stop=None,\n",
    "            frequency_penalty=None,\n",
    "            presence_penalty=None\n",
    "            )\n",
    "        \n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while querying the Hugging Face API: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_response_content_litellm(response):\n",
    "    \"\"\"\n",
    "    Extracts the content from the API response for serialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except (KeyError, TypeError) as e:\n",
    "        print(f\"Error extracting response content: {e}\")\n",
    "        print(\"Full response:\", response)\n",
    "    return None\n",
    "    \n",
    "def extract_response_content(response):\n",
    "    \"\"\"\n",
    "    Extracts the content from the API response for serialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return response.choices[0].message.content if response else None\n",
    "    except (KeyError, TypeError, AttributeError) as e:\n",
    "        print(f\"Error extracting response content: {e}\")\n",
    "        print(\"Full response:\", response)\n",
    "    return None\n",
    "\n",
    "def read_prompt_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the prompt from a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_responses_to_json(responses, folder_path, model, temperature, trial):\n",
    "    \"\"\"\n",
    "    Saves all responses to a single JSON file.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file_name = f\"output_responses_{model.replace('/', '_')}_temp{temperature}_{timestamp}_trial_{trial}.json\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    try:\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            json.dump(responses, file, indent=4)\n",
    "        print(f\"------------Responses saved to {output_file_path}---------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the file: {e}\")\n",
    "def save_response_to_json(response_content, folder_path, model, trial, filename):\n",
    "    \"\"\"\n",
    "    Saves the response content to a JSON file, ensuring no overwrite.\n",
    "    \"\"\"\n",
    "    output_file_name = f\"{filename.replace('.txt', '_response')}_{model.replace('/', '_')}_trial_{trial}.json\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    \n",
    "    if not os.path.exists(output_file_path):\n",
    "        try:\n",
    "            with open(output_file_path, 'w') as file:\n",
    "                json.dump({filename: response_content}, file, indent=4)\n",
    "            print(f\"Response saved to {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save the file: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping {filename}: Response file already exists.\")\n",
    "\n",
    "def process_file(filename, folder_path, model, temperature, trial, mode):\n",
    "    \"\"\"\n",
    "    Processes a single file, checking if a response already exists before querying.\n",
    "    \"\"\"\n",
    "    output_file_name = f\"{filename.replace('.txt', '_response')}_{model.replace('/', '_')}_trial_{trial}.json\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    \n",
    "    if os.path.exists(output_file_path):\n",
    "        print(f\"Skipping {filename}: Response already exists in {output_file_path}\")\n",
    "        return filename, None\n",
    "    \n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    prompt_text = read_prompt_from_file(file_path)\n",
    "    print(\"Prompt text for ... \", filename)\n",
    "    if prompt_text:\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        response = query_openai(prompt_text, model, temperature, mode)\n",
    "        if mode == \"bedrock\":\n",
    "            content = extract_response_content_litellm(response)\n",
    "        else:\n",
    "            content = extract_response_content(response)\n",
    "        \n",
    "        if content:\n",
    "            save_response_to_json(content, folder_path, model, trial, filename)\n",
    "            return filename, content\n",
    "    return filename, None\n",
    "\n",
    "def process_folder_parallel(folder_path, model, temperature, trial, mode, max_workers=10):\n",
    "    \"\"\"\n",
    "    Processes all eligible text files in the folder in parallel.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if re.match(r'output_sample_(\\d+)\\.txt', f)]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(process_file, file, folder_path, model, temperature, trial, mode): file for file in files}\n",
    "        for future in as_completed(future_to_file):\n",
    "            future.result()\n",
    "\n",
    "def combine_all_responses(folder_path, model, temperature, trial):\n",
    "    \"\"\"\n",
    "    Combines all response JSONs into a single JSON file only if every file \n",
    "    matching output_sample_{n}.txt has a corresponding \n",
    "    output_sample_{n}_response_{model}_trial_{trial}.json.\n",
    "    \"\"\"\n",
    "    responses = {}\n",
    "    pattern = re.compile(r\"^output_sample_(\\d+)\\.txt$\")  # Matches files like output_sample_1.txt\n",
    "\n",
    "    # Get all filenames in the folder\n",
    "    files_in_folder = set(os.listdir(folder_path))\n",
    "\n",
    "    # Collect all `output_sample_{n}.txt` and check if each has a corresponding JSON\n",
    "    txt_files = {file for file in files_in_folder if pattern.match(file)}\n",
    "    json_files = {\n",
    "        file.replace(\".txt\", f\"_response_{model.replace('/', '_')}_trial_{trial}.json\")\n",
    "        for file in txt_files\n",
    "    }\n",
    "\n",
    "    # Check if all expected JSON files exist\n",
    "    if not json_files.issubset(files_in_folder):\n",
    "        print(\">>>>>>>>>>>>>>>Not all .txt files have their corresponding .json files. Aborting combination.<<<<<<<<<<<<<<<\")\n",
    "        return\n",
    "\n",
    "    # If all JSON files exist, process them\n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(folder_path, json_file)\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                responses.update(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_file}: {e}\")\n",
    "    \n",
    "    save_responses_to_json(responses, folder_path, model, temperature, trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf6eb3f-67da-434a-92fe-70af782cd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b2c2ecb-8afb-41d3-9b50-377b896786d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before o1 1.0 0 o1 5\n",
      "Prompt text for ...  output_sample_27.txt\n",
      "Processing file: output_sample_27.txt\n",
      "Prompt text for ...  output_sample_32.txt\n",
      "Processing file: output_sample_32.txt\n",
      "Prompt text for ...  output_sample_33.txt\n",
      "Processing file: output_sample_33.txt\n",
      "Prompt text for ...  output_sample_26.txt\n",
      "Processing file: output_sample_26.txt\n",
      "Prompt text for ...  output_sample_18.txt\n",
      "Processing file: output_sample_18.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_32_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_30.txt\n",
      "Processing file: output_sample_30.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_27_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_24.txt\n",
      "Processing file: output_sample_24.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_26_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_25.txt\n",
      "Processing file: output_sample_25.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_18_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_31.txt\n",
      "Processing file: output_sample_31.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_33_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_19.txt\n",
      "Processing file: output_sample_19.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_31_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_35.txt\n",
      "Processing file: output_sample_35.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_25_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_21.txt\n",
      "Processing file: output_sample_21.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_30_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_20.txt\n",
      "Processing file: output_sample_20.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_24_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_34.txt\n",
      "Processing file: output_sample_34.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_35_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_22.txt\n",
      "Processing file: output_sample_22.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_19_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_36.txt\n",
      "Processing file: output_sample_36.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_20_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_37.txt\n",
      "Processing file: output_sample_37.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_21_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_23.txt\n",
      "Processing file: output_sample_23.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_36_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_44.txt\n",
      "Processing file: output_sample_44.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_37_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_50.txt\n",
      "Processing file: output_sample_50.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_34_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_51.txt\n",
      "Processing file: output_sample_51.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_22_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_45.txt\n",
      "Processing file: output_sample_45.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_44_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_53.txt\n",
      "Processing file: output_sample_53.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_53_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_46.txt\n",
      "Processing file: output_sample_46.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_50_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_52.txt\n",
      "Processing file: output_sample_52.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_23_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_56.txt\n",
      "Processing file: output_sample_56.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_51_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_42.txt\n",
      "Processing file: output_sample_42.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_52_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_43.txt\n",
      "Processing file: output_sample_43.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_56_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_57.txt\n",
      "Processing file: output_sample_57.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_42_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_8.txt\n",
      "Processing file: output_sample_8.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_45_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_41.txt\n",
      "Processing file: output_sample_41.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_57_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_55.txt\n",
      "Processing file: output_sample_55.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_43_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_69.txt\n",
      "Processing file: output_sample_69.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_46_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_68.txt\n",
      "Processing file: output_sample_68.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_8_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_54.txt\n",
      "Processing file: output_sample_54.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_41_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_40.txt\n",
      "Processing file: output_sample_40.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_55_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_9.txt\n",
      "Processing file: output_sample_9.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_69_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_4.txt\n",
      "Processing file: output_sample_4.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_40_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_59.txt\n",
      "Processing file: output_sample_59.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_68_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_65.txt\n",
      "Processing file: output_sample_65.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_54_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_70.txt\n",
      "Processing file: output_sample_70.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_4_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_64.txt\n",
      "Processing file: output_sample_64.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_9_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_58.txt\n",
      "Processing file: output_sample_58.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_65_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_5.txt\n",
      "Processing file: output_sample_5.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_59_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_7.txt\n",
      "Processing file: output_sample_7.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_64_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_66.txt\n",
      "Processing file: output_sample_66.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_70_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_67.txt\n",
      "Processing file: output_sample_67.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_5_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_6.txt\n",
      "Processing file: output_sample_6.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_58_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_2.txt\n",
      "Processing file: output_sample_2.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_67_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_63.txt\n",
      "Processing file: output_sample_63.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_7_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_62.txt\n",
      "Processing file: output_sample_62.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_6_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_3.txt\n",
      "Processing file: output_sample_3.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_63_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_1.txt\n",
      "Processing file: output_sample_1.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_62_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_60.txt\n",
      "Processing file: output_sample_60.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_66_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_48.txt\n",
      "Processing file: output_sample_48.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_1_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_49.txt\n",
      "Processing file: output_sample_49.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_2_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_61.txt\n",
      "Processing file: output_sample_61.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_60_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_12.txt\n",
      "Processing file: output_sample_12.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_48_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_13.txt\n",
      "Processing file: output_sample_13.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_61_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_39.txt\n",
      "Processing file: output_sample_39.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_49_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_11.txt\n",
      "Processing file: output_sample_11.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_13_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_10.txt\n",
      "Processing file: output_sample_10.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_3_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_38.txt\n",
      "Processing file: output_sample_38.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_39_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_14.txt\n",
      "Processing file: output_sample_14.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_12_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_28.txt\n",
      "Processing file: output_sample_28.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_11_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_29.txt\n",
      "Processing file: output_sample_29.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_10_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_15.txt\n",
      "Processing file: output_sample_15.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_15_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_17.txt\n",
      "Processing file: output_sample_17.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_14_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_16.txt\n",
      "Processing file: output_sample_16.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_29_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_38_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_28_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_16_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_sample_17_response_o1_trial_0.json\n",
      "------------Responses saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_before/output_responses_o1_temp1.0_20250213133841_trial_0.json---------\n",
      "/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after o1 1.0 0 o1 5\n",
      "Prompt text for ...  output_sample_27.txt\n",
      "Processing file: output_sample_27.txt\n",
      "Prompt text for ...  output_sample_26.txt\n",
      "Processing file: output_sample_26.txt\n",
      "Prompt text for ...  output_sample_33.txt\n",
      "Processing file: output_sample_33.txt\n",
      "Prompt text for ...  output_sample_32.txt\n",
      "Processing file: output_sample_32.txt\n",
      "Prompt text for ...  output_sample_18.txt\n",
      "Processing file: output_sample_18.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_33_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_30.txt\n",
      "Processing file: output_sample_30.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_27_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_24.txt\n",
      "Processing file: output_sample_24.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_26_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_25.txt\n",
      "Processing file: output_sample_25.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_32_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_31.txt\n",
      "Processing file: output_sample_31.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_30_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_19.txt\n",
      "Processing file: output_sample_19.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_24_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_35.txt\n",
      "Processing file: output_sample_35.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_18_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_21.txt\n",
      "Processing file: output_sample_21.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_35_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_20.txt\n",
      "Processing file: output_sample_20.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_31_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_34.txt\n",
      "Processing file: output_sample_34.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_25_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_22.txt\n",
      "Processing file: output_sample_22.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_19_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_36.txt\n",
      "Processing file: output_sample_36.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_22_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_37.txt\n",
      "Processing file: output_sample_37.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_34_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_23.txt\n",
      "Processing file: output_sample_23.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_20_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_44.txt\n",
      "Processing file: output_sample_44.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_44_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_50.txt\n",
      "Processing file: output_sample_50.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_21_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_51.txt\n",
      "Processing file: output_sample_51.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_23_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_45.txt\n",
      "Processing file: output_sample_45.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_36_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_53.txt\n",
      "Processing file: output_sample_53.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_50_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_46.txt\n",
      "Processing file: output_sample_46.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_53_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_52.txt\n",
      "Processing file: output_sample_52.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_37_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_56.txt\n",
      "Processing file: output_sample_56.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_52_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_42.txt\n",
      "Processing file: output_sample_42.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_46_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_43.txt\n",
      "Processing file: output_sample_43.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_51_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_57.txt\n",
      "Processing file: output_sample_57.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_56_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_8.txt\n",
      "Processing file: output_sample_8.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_42_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_41.txt\n",
      "Processing file: output_sample_41.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_43_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_55.txt\n",
      "Processing file: output_sample_55.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_8_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_69.txt\n",
      "Processing file: output_sample_69.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_45_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_68.txt\n",
      "Processing file: output_sample_68.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_57_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_54.txt\n",
      "Processing file: output_sample_54.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_41_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_40.txt\n",
      "Processing file: output_sample_40.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_55_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_9.txt\n",
      "Processing file: output_sample_9.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_68_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_4.txt\n",
      "Processing file: output_sample_4.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_69_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_59.txt\n",
      "Processing file: output_sample_59.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_40_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_65.txt\n",
      "Processing file: output_sample_65.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_59_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_70.txt\n",
      "Processing file: output_sample_70.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_54_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_64.txt\n",
      "Processing file: output_sample_64.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_9_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_58.txt\n",
      "Processing file: output_sample_58.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_58_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_5.txt\n",
      "Processing file: output_sample_5.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_65_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_7.txt\n",
      "Processing file: output_sample_7.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_4_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_66.txt\n",
      "Processing file: output_sample_66.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_5_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_67.txt\n",
      "Processing file: output_sample_67.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_64_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_6.txt\n",
      "Processing file: output_sample_6.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_70_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_2.txt\n",
      "Processing file: output_sample_2.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_6_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_63.txt\n",
      "Processing file: output_sample_63.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_7_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_62.txt\n",
      "Processing file: output_sample_62.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_66_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_3.txt\n",
      "Processing file: output_sample_3.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_2_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_1.txt\n",
      "Processing file: output_sample_1.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_63_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_60.txt\n",
      "Processing file: output_sample_60.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_67_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_48.txt\n",
      "Processing file: output_sample_48.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_62_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_49.txt\n",
      "Processing file: output_sample_49.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_60_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_61.txt\n",
      "Processing file: output_sample_61.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_48_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_12.txt\n",
      "Processing file: output_sample_12.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_49_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_13.txt\n",
      "Processing file: output_sample_13.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_61_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_39.txt\n",
      "Processing file: output_sample_39.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_3_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_11.txt\n",
      "Processing file: output_sample_11.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_11_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_10.txt\n",
      "Processing file: output_sample_10.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_39_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_38.txt\n",
      "Processing file: output_sample_38.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_13_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_14.txt\n",
      "Processing file: output_sample_14.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_10_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_28.txt\n",
      "Processing file: output_sample_28.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_12_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_29.txt\n",
      "Processing file: output_sample_29.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_1_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_15.txt\n",
      "Processing file: output_sample_15.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_29_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_17.txt\n",
      "Processing file: output_sample_17.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_15_response_o1_trial_0.json\n",
      "Prompt text for ...  output_sample_16.txt\n",
      "Processing file: output_sample_16.txt\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_14_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_28_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_17_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_38_response_o1_trial_0.json\n",
      "Response saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_sample_16_response_o1_trial_0.json\n",
      "------------Responses saved to /Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/cwe-476/exp_0_1/func_src_after/output_responses_o1_temp1.0_20250213134116_trial_0.json---------\n"
     ]
    }
   ],
   "source": [
    "cwe_list = [\"cwe-476\"]\n",
    "experiment_list = [\"exp_0_1\"] #, \"exp_0_2\", \"exp_15_2\", \"exp_16\", \"exp_17\", \"exp_17_inherent\", \"exp_18\", \"exp_15\"]\n",
    "#experiment_list = [\"exp_0_1\", \"exp_0_2\", \"exp_15_2\", \"exp_16\", \"exp_17\", \"exp_17_inherent\", \"exp_18\", \"exp_15\"]\n",
    "\n",
    "for cwe in cwe_list:\n",
    "    for exp in experiment_list:\n",
    "        folder_path_before = f\"/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/{cwe}/{exp}/func_src_before\"\n",
    "        folder_path_after = f\"/Users/ic/Desktop/LLM-as-Static-Proxy-Test/experiments_latest/{cwe}/{exp}/func_src_after\"\n",
    "        #model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
    "        model = \"o1\"\n",
    "        #model = \"bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "        #model = \"deepseek-ai/DeepSeek-R1\"\n",
    "        temperature = 1.0\n",
    "        trial = 0\n",
    "        mw = 10\n",
    "        mode = \"openai\"\n",
    "        if model == \"deepseek-ai/DeepSeek-R1\":\n",
    "            mode = \"deepseek-r1-together\"\n",
    "            mw = 100\n",
    "        elif model == \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\":\n",
    "            mode = \"hf_endpoint\"\n",
    "            mw = 4\n",
    "        elif model == \"o1\":\n",
    "            mode = \"o1\"\n",
    "            mw = 5\n",
    "        elif model == \"bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0\":\n",
    "            print(\"yes......\")\n",
    "            mode = \"bedrock\"\n",
    "            mw = 10\n",
    "\n",
    "        print(folder_path_before, model, temperature, trial, mode, mw)\n",
    "        process_folder_parallel(folder_path_before, model, temperature, trial, mode, max_workers=mw)\n",
    "        combine_all_responses(folder_path_before, model, temperature, trial)\n",
    "\n",
    "        print(folder_path_after, model, temperature, trial, mode, mw)\n",
    "        process_folder_parallel(folder_path_after, model, temperature, trial, mode, max_workers=mw)\n",
    "        combine_all_responses(folder_path_after, model, temperature, trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a57d2-d602-4df3-9b3b-b997a35e2335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf19d29-fb04-4a98-aba6-7c18973fb0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e7207-6241-49e4-bd05-e4faa3148027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, install the openai Python library by running \n",
    "# pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://ue4xb2y39sfo803n.us-east-1.aws.endpoints.huggingface.cloud/v1/\", \n",
    "\tapi_key=\"\" \n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "\tmodel=\"tgi\",\n",
    "\tmessages=[\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is deep learning?\"\n",
    "\t}\n",
    "],\n",
    "\ttop_p=None,\n",
    "\ttemperature=None,\n",
    "\tmax_tokens=150,\n",
    "\tstream=False,\n",
    "\tseed=None,\n",
    "\tstop=None,\n",
    "\tfrequency_penalty=None,\n",
    "\tpresence_penalty=None\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)\n",
    "#for message in chat_completion:\n",
    "#\tprint(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7568d-9e45-4793-b224-c920ecfd2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message.choices[0].delta.content, end=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ab527-ec20-4b79-99f9-cba883d894ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(prompt, model, temperature, mode):\n",
    "    \"\"\"\n",
    "    Queries the OpenAI API with the specified parameters and returns the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai\n",
    "\n",
    "        if mode == \"deepseek\":\n",
    "            client = OpenAI(\n",
    "                base_url=\"https://huggingface.co/api/inference-proxy/together\",\n",
    "                api_key=\"\"\n",
    "            )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant. You must LIMIT YOUR RESPONSE TO 130 TOKENS.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=130,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stream=False\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during API query: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_response_content(response):\n",
    "    \"\"\"\n",
    "    Extracts the content from the API response for serialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return response.choices[0].message.content\n",
    "    except (KeyError, TypeError) as e:\n",
    "        print(f\"Error extracting response content: {e}\")\n",
    "        print(\"Full response:\", response)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0174faa-73d6-465d-b6fa-a5e1611699cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =\"deepseek-ai/DeepSeek-R1\"\n",
    "mode = \"deepseek\"\n",
    "temperature = 0.7\n",
    "response = query_openai(\"What is the capital of France?\", model, temperature, mode)\n",
    "content = extract_response_content(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b5c2a-adf7-4655-8883-e24607c6c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac72a8c-eac9-4861-9972-88b3a249938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://huggingface.co/api/inference-proxy/together\",\n",
    "\tapi_key=\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "\tmodel=\"deepseek-ai/DeepSeek-R1\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a8b24-f5bf-4782-a41f-3d0f729a3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027c726-dc7b-4277-94ce-9a1999df63b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
