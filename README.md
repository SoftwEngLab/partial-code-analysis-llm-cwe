# Partial Code Analysis with LLMs for CWE-Specific Vulnerabilities

This project provides a framework for evaluating large language models (LLMs) on partial code snippets, using **CWE-specific rules** (Common Weakness Enumeration) to assess vulnerability understanding and reasoning.

---

## üìÇ Repo Structure

- `run_main.sh` ‚Äî Main pipeline entry point.
- `run_eval_analysis.sh` ‚Äî Evaluate LLM responses with custom analysis.
- `analyze_results.py` ‚Äî Script to parse and process model outputs.
- `eval_run_experiments.sh` ‚Äî Generates `eval_*.json` files from model outputs.
- `run_create_plots.sh` ‚Äî Visualizes evaluation results using various plotting scripts (e.g., `plot_results.py`, `plot_average_performance_all.py`).

---

## üß™ Example Usage

### Run the Main Evaluation
```bash
bash run_main.sh
```

### Run Evaluation Analysis
```bash
bash run_eval_analysis.sh <CWE-ID> <EXPERIMENT_IDS> <MODEL_NAME> ./analyze_results.py <OPTIONAL_SEED>
```

- `<CWE-ID>`: e.g., `"cwe-078"`, `"cwe-190"`, `"cwe-416"`, `"cwe-476"`
- `<EXPERIMENT_IDS>`: Comma-separated list, e.g., `"exp_0_1,exp_16,exp_17"`
- `<MODEL_NAME>`: e.g., `mistral-large@2407`, `deepseek-ai_DeepSeek-R1`, `bedrock_us.anthropic.claude-3-5-sonnet-20241022-v2:0`, `o1`
- `<OPTIONAL_SEED>`: Integer controlling trial variation (optional)

### Example:
```bash
bash run_eval_analysis.sh "cwe-190" "exp_0_1,exp_16" "deepseek-ai_DeepSeek-R1" ./analyze_results.py 1
```

---

## üìä Creating Evaluation Files/Plots

### Step 1: Generate Evaluation Files
```bash
bash eval_run_experiments.sh
```

### Step 2: Plot Results
```bash
# Plot overall average
bash run_create_plots.sh "partial-code-analysis-llm-cwe/data/trial_results" ./plot_average_performance_all.py

# Plot individual model performance
bash run_create_plots.sh "partial-code-analysis-llm-cwe/data/trial_results" ./plot_results.py
```

---

## üß† Notes

- Ensure your environment has access to the necessary model APIs.
- Set your OpenAI API key if using OpenAI models:
```bash
export OPENAI_API_KEY=sk-proj-...
```

- For Generic Chain-of-Thought (CoT) experiments, use:
```bash
exp_25
```

---

### Experiment Prompt Mapping

| Prompt / Description                                                                                      | Experiment ID     |
|------------------------------------------------------------------------------------------------------------|-------------------|
| ‚ÄúIs the new example vulnerable or non-vulnerable?‚Äù                                                                    | `exp_0_1`         |
| ‚ÄúIs the new example vulnerable to < CWE-ID > ?‚Äù (no NL or CoT)                                                | `exp_0_2`         |
| NL instruction generated by the LLM                                                                        | `exp_15`          |
| NL generated with 3 few-shot examples from SVEN                                                            | `exp_15_2`        |
| NL from MITRE CWE Dictionary                                                                               | `exp_16`          |
| NL as ‚Äútests‚Äù + contrastive CoT examples generated by LLM                                                  | `exp_17_inherent` |
| NL from MITRE + contrastive CoT based on demonstratives                                                    | `exp_17`          |
| Free-form NL + contrastive CoT generated by the LLM                                                        | `exp_18`          |
| Generic CoT                                                                                                | `exp_25`          |
